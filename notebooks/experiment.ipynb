{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "import numpy as np\n",
    "import copy\n",
    "import statistics\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "import sys\n",
    "sys.path.append('./../src')\n",
    "import globals\n",
    "from model import Net\n",
    "from training import train_model, train_model_CL\n",
    "from visualizations import plot_embeddings, plot_confusion_matrix\n",
    "from feature_attribution import Feature_Importance_Evaluations\n",
    "from pytorch_utils import get_features, get_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ITERATIONS = globals.ITERATIONS\n",
    "CLASSES_PER_ITER = globals.CLASSES_PER_ITER\n",
    "SEED = globals.SEED\n",
    "DEVICE = globals.DEVICE\n",
    "full_trainset = globals.full_trainset\n",
    "trainset = globals.trainset\n",
    "testset = globals.testset\n",
    "trainloaders = globals.trainloaders\n",
    "valloaders = globals.valloaders\n",
    "testloaders = globals.testloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the two-step process used to prepare the\n",
    "# data for use with the convolutional neural network.\n",
    "\n",
    "# First step is to convert Python Image Library (PIL) format\n",
    "# to PyTorch tensors.\n",
    "\n",
    "# Second step is used to normalize the data by specifying a \n",
    "# mean and standard deviation for each of the three channels.\n",
    "# This will convert the data from [0,1] to [-1,1]\n",
    "\n",
    "# Normalization of data should help speed up conversion and\n",
    "# reduce the chance of vanishing gradients with certain \n",
    "# activation functions.\n",
    "def initialize_data():\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor()\n",
    "        #transforms.Normalize((0.5,), (0.5,))  # Normalizes to mean 0.5 and std 0.5 for the single channel\n",
    "    ])\n",
    "\n",
    "    globals.full_trainset = torchvision.datasets.MNIST('./../data/', train=True, download=True,\n",
    "                                transform=transform)\n",
    "    targets = np.array(globals.full_trainset.targets)\n",
    "\n",
    "    # Perform stratified split\n",
    "    train_indices, val_indices = train_test_split(\n",
    "        np.arange(len(targets)),\n",
    "        test_size=0.15,\n",
    "        stratify=targets\n",
    "    )\n",
    "\n",
    "    # Create subsets\n",
    "    valset = Subset(globals.full_trainset, val_indices)\n",
    "    globals.trainset = Subset(globals.full_trainset, train_indices)\n",
    "\n",
    "    globals.testset = torchvision.datasets.MNIST('./../data/', train=False, download=True,\n",
    "                                transform=transform)\n",
    "\n",
    "    # Define class pairs for each subset\n",
    "    class_pairs = [tuple(range(i*CLASSES_PER_ITER,(i+1)*CLASSES_PER_ITER)) for i in range(ITERATIONS)]\n",
    "    #print(class_pairs)\n",
    "\n",
    "    # Dictionary to hold data loaders for each subset\n",
    "    globals.trainloaders = []\n",
    "    globals.testloaders = []\n",
    "    globals.valloaders = []\n",
    "    subset_indices = []\n",
    "    # Loop over each class pair\n",
    "    for i, t in enumerate(class_pairs):\n",
    "        # Get indices of images belonging to the specified class pair\n",
    "        subs_ind = [idx for idx, (_, label) in enumerate(globals.trainset) if label in list(t)]\n",
    "        val_subset_indices = [idx for idx, (_, label) in enumerate(valset) if label in list(t)]\n",
    "        test_subset_indices = [idx for idx, (_, label) in enumerate(globals.testset) if label in list(t)]\n",
    "        # Create a subset for the current class pair\n",
    "        train_subset = Subset(globals.trainset, subs_ind)\n",
    "        globals.trainloaders.append(DataLoader(train_subset, batch_size=globals.BATCH_SIZE, shuffle=True, pin_memory=True, num_workers = 0))\n",
    "\n",
    "        subset_indices.append(subs_ind)\n",
    "        \n",
    "        val_subset = Subset(valset, val_subset_indices)\n",
    "        globals.valloaders.append(DataLoader(val_subset, batch_size=500, shuffle=False))\n",
    "\n",
    "        test_subset = Subset(globals.testset, test_subset_indices)\n",
    "        globals.testloaders.append(DataLoader(test_subset, batch_size=500, shuffle=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiment(verbose = False):\n",
    "    def _print(*args, **kwargs):\n",
    "        if verbose:\n",
    "            print(*args, **kwargs)\n",
    "    initialize_data()\n",
    "    prevModel = None\n",
    "    globals.OOD_CLASS=1\n",
    "    globals.BATCH_SIZE=4\n",
    "\n",
    "    #[Denis] added code:\n",
    "    Feature_Importance_Eval=Feature_Importance_Evaluations(globals.valloaders, DEVICE)\n",
    "\n",
    "    for i in tqdm(range(ITERATIONS), desc=\"Experiment Progress\"):\n",
    "        model = Net((i+1)*(CLASSES_PER_ITER+globals.OOD_CLASS), withDropout=True)\n",
    "        if prevModel is not None:\n",
    "            with torch.no_grad():\n",
    "                model.copyPrev(prevModel)\n",
    "        train_loader = globals.trainloaders[i]\n",
    "        val_loader = globals.valloaders[i]\n",
    "        if prevModel:\n",
    "            _print(\"CL TRAIN!!\")\n",
    "            train_model_CL(\n",
    "                model,\n",
    "                prevModel,\n",
    "                train_loader,\n",
    "                val_loader,\n",
    "                i,\n",
    "                verbose,\n",
    "                10000,\n",
    "                True,\n",
    "                freeze_nonzero_params=False,\n",
    "                l1_loss=0,\n",
    "                ewc_loss=0,\n",
    "                kd_loss=1,\n",
    "                distance_loss=0,\n",
    "                center_loss=0,\n",
    "                param_reuse_loss=0,\n",
    "                stopOnLoss=None,\n",
    "                stopOnValAcc=0.92,\n",
    "                )\n",
    "        else:\n",
    "            train_model(\n",
    "                model, \n",
    "                train_loader, \n",
    "                val_loader, \n",
    "                verbose, \n",
    "                epochs=10000, \n",
    "                l1_loss=0,\n",
    "                stopOnLoss=0.05,\n",
    "                center_loss =0,\n",
    "                )\n",
    "\n",
    "        #[Denis] added code:\n",
    "        Feature_Importance_Eval.Task_Feature_Attribution(model, i)\n",
    "        \n",
    "        if verbose or i == ITERATIONS-1:\n",
    "            _print(\"ITERATION\", i+1)\n",
    "            _print(\"ACCURACIES PER TASK:\")\n",
    "            print(\"Starting evaluation\")\n",
    "            accumPred = []\n",
    "            all_labels = []\n",
    "            with torch.no_grad():\n",
    "                for j in range(i+1):\n",
    "                    val_loader = globals.testloaders[j]\n",
    "                    val_labels = get_labels(val_loader).to(DEVICE)\n",
    "                    all_labels.append(val_labels)\n",
    "                    model.eval()\n",
    "                    pred = model(get_features(val_loader).to(DEVICE))\n",
    "                    model.train()\n",
    "                    accumPred.append(pred)\n",
    "                    sliced_pred = pred[:, j*(CLASSES_PER_ITER+globals.OOD_CLASS):(j+1)*(CLASSES_PER_ITER+globals.OOD_CLASS)]\n",
    "                    _, predicted = torch.max(sliced_pred, 1)  # Get the class predictions\n",
    "                    predicted += j*CLASSES_PER_ITER\n",
    "                    correct = (predicted == val_labels).sum().item()  # Count how many were correct\n",
    "                    accuracy = correct / val_labels.size(0)  # Accuracy as a percentage\n",
    "                    _print(str(accuracy), end=' ')\n",
    "            accumPred = torch.cat(accumPred)\n",
    "            all_labels = torch.cat(all_labels)\n",
    "            predicted = []\n",
    "            for x in accumPred:\n",
    "                if globals.OOD_CLASS == 1:\n",
    "                    x_pred = x[[i for i in range(x.size(0)) if (i + 1) % (CLASSES_PER_ITER+1) != 0]]\n",
    "                else:\n",
    "                    x_pred = x\n",
    "                x_pred = torch.softmax(x_pred, dim=-1)\n",
    "                max = 0\n",
    "                for (k, v) in enumerate(x_pred):\n",
    "                    if v > max:\n",
    "                        max = v\n",
    "                        p = k\n",
    "                predicted.append(p)\n",
    "            predicted = torch.tensor(predicted).to(DEVICE)\n",
    "            correct = (predicted == all_labels).sum().item()  # Count how many were correct\n",
    "            accuracy = correct / all_labels.size(0)  # Accuracy as a percentage\n",
    "            _print(\"Accuracy on tasks so far:\", accuracy)\n",
    "            if verbose:\n",
    "                plot_confusion_matrix(predicted.cpu(), all_labels.cpu(), list(range(CLASSES_PER_ITER*(i+1))))\n",
    "        prevModel = copy.deepcopy(model)\n",
    "        \n",
    "    #[Denis] added code:\n",
    "    avg_shap_vals,shap_vals=Feature_Importance_Eval.Get_Feature_Change_Score(prevModel)\n",
    "    _print(\"Average SHAPC values (ordered as tasks):\", shap_vals)\n",
    "    _print(\"Averaged SHAPC value:\", avg_shap_vals)\n",
    "    \n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiments(n_runs=1, verbose=True):\n",
    "    def _print(*args, **kwargs):\n",
    "        if verbose:\n",
    "            print(*args, **kwargs)\n",
    "    accuracies = []\n",
    "    for r in range(n_runs):\n",
    "        print(f\"Starting run {r+1}.\")\n",
    "        acc = run_experiment(verbose)\n",
    "        accuracies.append(acc)\n",
    "        _print(f\"Run {r} finished with accuracy {acc}\")\n",
    "    mean_acc = statistics.mean(accuracies)\n",
    "    acc_std = statistics.stdev(accuracies)\n",
    "    print(\"Accuracies:\")\n",
    "    print(accuracies)\n",
    "    print(f\"Mean accuracy across {n_runs} runs: {mean_acc}\")\n",
    "    print(f\"Standard deviation of accuracy across {n_runs} runs: {acc_std}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_experiments(n_runs=2, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_experiment(verbose=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl_project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
