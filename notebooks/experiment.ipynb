{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "import numpy as np\n",
    "import copy\n",
    "import statistics\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "import sys\n",
    "sys.path.append('./../src')\n",
    "import globals\n",
    "from model import Net\n",
    "from training import train_model, train_model_CL\n",
    "from visualizations import plot_embeddings, plot_confusion_matrix\n",
    "from feature_attribution import Feature_Importance_Evaluations\n",
    "from pytorch_utils import get_features, get_labels\n",
    "from embedding_measurements import measure_embedding_confusion_knn, measure_embedding_drift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ITERATIONS = globals.ITERATIONS\n",
    "CLASSES_PER_ITER = globals.CLASSES_PER_ITER\n",
    "SEED = globals.SEED\n",
    "DEVICE = globals.DEVICE\n",
    "full_trainset = globals.full_trainset\n",
    "trainset = globals.trainset\n",
    "testset = globals.testset\n",
    "trainloaders = globals.trainloaders\n",
    "valloaders = globals.valloaders\n",
    "testloaders = globals.testloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the two-step process used to prepare the\n",
    "# data for use with the convolutional neural network.\n",
    "\n",
    "# First step is to convert Python Image Library (PIL) format\n",
    "# to PyTorch tensors.\n",
    "\n",
    "# Second step is used to normalize the data by specifying a \n",
    "# mean and standard deviation for each of the three channels.\n",
    "# This will convert the data from [0,1] to [-1,1]\n",
    "\n",
    "# Normalization of data should help speed up conversion and\n",
    "# reduce the chance of vanishing gradients with certain \n",
    "# activation functions.\n",
    "def initialize_data():\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor()\n",
    "        #transforms.Normalize((0.5,), (0.5,))  # Normalizes to mean 0.5 and std 0.5 for the single channel\n",
    "    ])\n",
    "\n",
    "    globals.full_trainset = torchvision.datasets.MNIST('./../data/', train=True, download=True,\n",
    "                                transform=transform)\n",
    "    targets = np.array(globals.full_trainset.targets)\n",
    "\n",
    "    # Perform stratified split\n",
    "    train_indices, val_indices = train_test_split(\n",
    "        np.arange(len(targets)),\n",
    "        test_size=0.15,\n",
    "        stratify=targets\n",
    "    )\n",
    "\n",
    "    # Create subsets\n",
    "    valset = Subset(globals.full_trainset, val_indices)\n",
    "    globals.trainset = Subset(globals.full_trainset, train_indices)\n",
    "\n",
    "    globals.testset = torchvision.datasets.MNIST('./../data/', train=False, download=True,\n",
    "                                transform=transform)\n",
    "\n",
    "    # Define class pairs for each subset\n",
    "    class_pairs = [tuple(range(i*CLASSES_PER_ITER,(i+1)*CLASSES_PER_ITER)) for i in range(ITERATIONS)]\n",
    "    #print(class_pairs)\n",
    "\n",
    "    # Dictionary to hold data loaders for each subset\n",
    "    globals.trainloaders = []\n",
    "    globals.testloaders = []\n",
    "    globals.valloaders = []\n",
    "    subset_indices = []\n",
    "    # Loop over each class pair\n",
    "    for i, t in enumerate(class_pairs):\n",
    "        # Get indices of images belonging to the specified class pair\n",
    "        subs_ind = [idx for idx, (_, label) in enumerate(globals.trainset) if label in list(t)]\n",
    "        val_subset_indices = [idx for idx, (_, label) in enumerate(valset) if label in list(t)]\n",
    "        test_subset_indices = [idx for idx, (_, label) in enumerate(globals.testset) if label in list(t)]\n",
    "        # Create a subset for the current class pair\n",
    "        train_subset = Subset(globals.trainset, subs_ind)\n",
    "        globals.trainloaders.append(DataLoader(train_subset, batch_size=globals.BATCH_SIZE, shuffle=True, pin_memory=True, num_workers = 0))\n",
    "\n",
    "        subset_indices.append(subs_ind)\n",
    "        \n",
    "        val_subset = Subset(valset, val_subset_indices)\n",
    "        globals.valloaders.append(DataLoader(val_subset, batch_size=500, shuffle=False))\n",
    "\n",
    "        test_subset = Subset(globals.testset, test_subset_indices)\n",
    "        globals.testloaders.append(DataLoader(test_subset, batch_size=500, shuffle=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiment(\n",
    "        verbose = False,\n",
    "        stopOnLoss = 0.03,\n",
    "        full_CE = True,\n",
    "        with_OOD = False,\n",
    "        kd_loss = 0,\n",
    "        stopOnValAcc = None,\n",
    "        epochs = 1000000,\n",
    "        with_dropout = False\n",
    "        ):\n",
    "    def _print(*args, **kwargs):\n",
    "        if verbose:\n",
    "            print(*args, **kwargs)\n",
    "    if with_OOD:\n",
    "        globals.OOD_CLASS = 1\n",
    "    else:\n",
    "        globals.OOD_CLASS = 0\n",
    "    initialize_data()\n",
    "    prevModel = None\n",
    "    globals.OOD_CLASS=1\n",
    "    globals.BATCH_SIZE=4\n",
    "    globals.WITH_DROPOUT = with_dropout\n",
    "\n",
    "    #[Denis] added code:\n",
    "    Feature_Importance_Eval=Feature_Importance_Evaluations(globals.valloaders, DEVICE)\n",
    "\n",
    "    for i in tqdm(range(ITERATIONS), desc=\"Experiment Progress\"):\n",
    "        model = Net((i+1)*(CLASSES_PER_ITER+globals.OOD_CLASS))\n",
    "        if prevModel is not None:\n",
    "            with torch.no_grad():\n",
    "                model.copyPrev(prevModel)\n",
    "        train_loader = globals.trainloaders[i]\n",
    "        val_loader = globals.valloaders[i]\n",
    "        if prevModel:\n",
    "            _print(\"CL TRAIN!!\")\n",
    "            train_model_CL(\n",
    "                model,\n",
    "                prevModel,\n",
    "                train_loader,\n",
    "                val_loader,\n",
    "                i,\n",
    "                verbose,\n",
    "                epochs,\n",
    "                True,\n",
    "                freeze_nonzero_params=False,\n",
    "                l1_loss=0,\n",
    "                ewc_loss=0,\n",
    "                kd_loss=kd_loss,\n",
    "                distance_loss=0,\n",
    "                center_loss=0,\n",
    "                param_reuse_loss=0,\n",
    "                stopOnLoss=stopOnLoss,\n",
    "                stopOnValAcc = stopOnValAcc,\n",
    "                full_CE=full_CE\n",
    "                )\n",
    "        else:\n",
    "            train_model(\n",
    "                model, \n",
    "                train_loader, \n",
    "                val_loader, \n",
    "                verbose, \n",
    "                epochs=epochs, \n",
    "                l1_loss=0,\n",
    "                stopOnLoss=stopOnLoss,\n",
    "                center_loss =0,\n",
    "                )\n",
    "\n",
    "        #[Denis] added code:\n",
    "        Feature_Importance_Eval.Task_Feature_Attribution(model, i)\n",
    "        \n",
    "        if verbose or i == ITERATIONS-1:\n",
    "            _print(\"Starting evaluation\")\n",
    "            _print(\"ITERATION\", i+1)\n",
    "            _print(\"ACCURACIES PER TASK:\")\n",
    "            accumPred = []\n",
    "            all_labels = []\n",
    "            all_embeddings = []\n",
    "            with torch.no_grad():\n",
    "                for j in range(i+1):\n",
    "                    val_loader = globals.testloaders[j]\n",
    "                    val_labels = get_labels(val_loader).to(DEVICE)\n",
    "                    all_labels.append(val_labels)\n",
    "                    model.eval()\n",
    "                    pred, embeddings = model.get_pred_and_embeddings((get_features(val_loader).to(DEVICE)))\n",
    "                    model.train()\n",
    "                    accumPred.append(pred)\n",
    "                    all_embeddings.append(embeddings)\n",
    "                    sliced_pred = pred[:, j*(CLASSES_PER_ITER+globals.OOD_CLASS):(j+1)*(CLASSES_PER_ITER+globals.OOD_CLASS)]\n",
    "                    _, predicted = torch.max(sliced_pred, 1)  # Get the class predictions\n",
    "                    predicted += j*CLASSES_PER_ITER\n",
    "                    correct = (predicted == val_labels).sum().item()  # Count how many were correct\n",
    "                    accuracy = correct / val_labels.size(0)  # Accuracy as a percentage\n",
    "                    _print(str(accuracy), end=' ')\n",
    "            accumPred = torch.cat(accumPred)\n",
    "            all_labels = torch.cat(all_labels)\n",
    "            all_embeddings = torch.cat(all_embeddings)\n",
    "            predicted = []\n",
    "            for x in accumPred:\n",
    "                if globals.OOD_CLASS == 1:\n",
    "                    x_pred = x[[i for i in range(x.size(0)) if (i + 1) % (CLASSES_PER_ITER+1) != 0]]\n",
    "                else:\n",
    "                    x_pred = x\n",
    "                x_pred = torch.softmax(x_pred, dim=-1)\n",
    "                max = 0\n",
    "                for (k, v) in enumerate(x_pred):\n",
    "                    if v > max:\n",
    "                        max = v\n",
    "                        p = k\n",
    "                predicted.append(p)\n",
    "            predicted = torch.tensor(predicted).to(DEVICE)\n",
    "            correct = (predicted == all_labels).sum().item()  # Count how many were correct\n",
    "            accuracy = correct / all_labels.size(0)  # Accuracy as a percentage\n",
    "            _print(\"Accuracy on tasks so far:\", accuracy)\n",
    "\n",
    "            embedding_drift = measure_embedding_drift(all_embeddings, all_labels, model.prev_test_embedding_centers)\n",
    "            _print(\"Average embedding drift based on centroids:\", embedding_drift)\n",
    "            total_confusion, intra_phase_confusion, per_task_confusion = measure_embedding_confusion_knn(all_embeddings, all_labels, k = 500, task=i+1)\n",
    "            _print(\"Total confusion\", total_confusion)\n",
    "            _print(\"Intra-phase confusion\", intra_phase_confusion)\n",
    "            _print(\"Per task confusions\", per_task_confusion)\n",
    "            if verbose:\n",
    "                plot_confusion_matrix(predicted.cpu(), all_labels.cpu(), list(range(CLASSES_PER_ITER*(i+1))))\n",
    "        prevModel = copy.deepcopy(model)\n",
    "        \n",
    "    #[Denis] added code:\n",
    "    avg_shap_val,shap_vals=Feature_Importance_Eval.Get_Feature_Change_Score(prevModel)\n",
    "    _print(\"Average SHAPC values (ordered as tasks):\", shap_vals)\n",
    "    _print(\"Averaged SHAPC value:\", avg_shap_val)\n",
    "    \n",
    "    return accuracy, total_confusion, intra_phase_confusion, per_task_confusion, embedding_drift, avg_shap_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiments(n_runs=1, *args, **kwargs):\n",
    "    verbose = kwargs.get('verbose', None)\n",
    "    def _print(*args, **kwargs):\n",
    "        if verbose:\n",
    "            print(*args, **kwargs)\n",
    "    accuracies = []\n",
    "    total_confusions = []\n",
    "    intra_phase_confusions = []\n",
    "    per_task_confusions = []\n",
    "    shap_vals = []\n",
    "    embedding_drifts = []\n",
    "    for r in range(n_runs):\n",
    "        print(f\"Starting run {r+1}.\")\n",
    "        accuracy, total_confusion, intra_phase_confusion, per_task_confusion, embedding_drift, avg_shap_val = run_experiment(*args, **kwargs)\n",
    "        accuracies.append(accuracy)\n",
    "        total_confusions.append(total_confusion)\n",
    "        intra_phase_confusions.append(intra_phase_confusion)\n",
    "        per_task_confusions.append(per_task_confusion)\n",
    "        shap_vals.append(avg_shap_val)\n",
    "        embedding_drifts.append(embedding_drift)\n",
    "        _print(f\"Run {r} finished with accuracy {accuracy}\")\n",
    "    # Calculate means and standard deviations for each measure\n",
    "    mean_acc = statistics.mean(accuracies)\n",
    "    acc_std = statistics.stdev(accuracies)\n",
    "\n",
    "    mean_total_confusion = statistics.mean(total_confusions)\n",
    "    total_confusion_std = statistics.stdev(total_confusions)\n",
    "\n",
    "    mean_intra_phase_confusion = statistics.mean(intra_phase_confusions)\n",
    "    intra_phase_confusion_std = statistics.stdev(intra_phase_confusions)\n",
    "\n",
    "    mean_per_task_confusion = statistics.mean(per_task_confusions)\n",
    "    per_task_confusion_std = statistics.stdev(per_task_confusions)\n",
    "\n",
    "    mean_embedding_drift = statistics.mean(embedding_drifts)\n",
    "    embedding_drift_std = statistics.stdev(embedding_drifts)\n",
    "\n",
    "    mean_shap_val = statistics.mean(shap_vals)\n",
    "    shap_val_std = statistics.stdev(shap_vals)\n",
    "\n",
    "    # Print all results\n",
    "    #print(\"Accuracies:\")\n",
    "    #print(accuracies)\n",
    "    print(f\"Mean accuracy across {n_runs} runs: {mean_acc}\")\n",
    "    print(f\"Standard deviation of accuracy across {n_runs} runs: {acc_std}\\n\")\n",
    "\n",
    "    #print(\"Total Confusions:\")\n",
    "    #print(total_confusions)\n",
    "    print(f\"Mean total confusion across {n_runs} runs: {mean_total_confusion}\")\n",
    "    print(f\"Standard deviation of total confusion across {n_runs} runs: {total_confusion_std}\\n\")\n",
    "\n",
    "    #print(\"Intra-Phase Confusions:\")\n",
    "    #print(intra_phase_confusions)\n",
    "    print(f\"Mean intra-phase confusion across {n_runs} runs: {mean_intra_phase_confusion}\")\n",
    "    print(f\"Standard deviation of intra-phase confusion across {n_runs} runs: {intra_phase_confusion_std}\\n\")\n",
    "\n",
    "    #print(\"Per-Task Confusions:\")\n",
    "    #print(per_task_confusions)\n",
    "    print(f\"Mean per-task confusion across {n_runs} runs: {mean_per_task_confusion}\")\n",
    "    print(f\"Standard deviation of per-task confusion across {n_runs} runs: {per_task_confusion_std}\\n\")\n",
    "\n",
    "    #print(\"Embedding drifts:\")\n",
    "    #print(embedding_drifts)\n",
    "    print(f\"Mean embedding drift across {n_runs} runs: {mean_embedding_drift}\")\n",
    "    print(f\"Standard deviation of embedding drift across {n_runs} runs: {embedding_drift_std}\\n\")\n",
    "\n",
    "    #print(\"SHAP Values:\")\n",
    "    #print(shap_vals)\n",
    "    print(f\"Mean SHAP values across {n_runs} runs: {mean_shap_val}\")\n",
    "    print(f\"Standard deviation of SHAP values across {n_runs} runs: {shap_val_std}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_experiments(n_runs=5, verbose=False, stopOnLoss = 0.02)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_experiments(n_runs=5, verbose=False, stopOnLoss = 0.02, with_dropout=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_experiments(n_runs=5, verbose=False, stopOnLoss = 0.02, kd_loss=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_experiments(n_runs=5, verbose=False, stopOnLoss = 0.02, full_CE=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_experiments(n_runs=5, verbose=False, stopOnLoss = 0.02, with_OOD=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_experiments(n_runs=5, verbose=False, stopOnLoss = 0.02, full_CE = False, with_OOD=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_experiments(n_runs=5, verbose=False, stopOnLoss = 0.02, full_CE = False, kd_loss = 1, with_OOD=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_experiments(n_runs=5, verbose=False, stopOnLoss = 0.02, full_CE = False, kd_loss = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_experiments(n_runs=5, verbose=False, stopOnLoss = 0.02, full_CE = False, kd_loss = 1, with_dropout=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_experiments(n_runs=5, verbose=False, stopOnLoss = 0.02, full_CE = False, kd_loss = 1, with_OOD=True, with_dropout=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
