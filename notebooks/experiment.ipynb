{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torchvision import transforms, datasets\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "import numpy as np\n",
    "import copy\n",
    "import statistics\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "torch.cuda.empty_cache()\n",
    "import sys\n",
    "sys.path.append('./../src')\n",
    "import globals\n",
    "from model import MnistCNN, TinyImageNetCNN\n",
    "from training import train_model, train_model_CL\n",
    "from visualizations import plot_embeddings, plot_confusion_matrix\n",
    "from feature_attribution import Feature_Importance_Evaluations\n",
    "from pytorch_utils import get_features, get_labels\n",
    "from embedding_measurements import measure_embedding_confusion_knn, measure_embedding_drift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = globals.SEED\n",
    "DEVICE = globals.DEVICE\n",
    "full_trainset = globals.full_trainset\n",
    "trainset = globals.trainset\n",
    "testset = globals.testset\n",
    "trainloaders = globals.trainloaders\n",
    "valloaders = globals.valloaders\n",
    "testloaders = globals.testloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import json\n",
    "file_path = \"class_to_indices.json\"\n",
    "def initialize_data():\n",
    "    data_folder = './../data/'\n",
    "    if globals.dataset == 'mnist':\n",
    "        transform = transforms.Compose([\n",
    "            transforms.ToTensor()\n",
    "            #transforms.Normalize((0.5,), (0.5,))  # Normalizes to mean 0.5 and std 0.5 for the single channel\n",
    "        ])\n",
    "        globals.full_trainset = torchvision.datasets.MNIST(data_folder, train=True, download=True,\n",
    "                                    transform=transform)\n",
    "        targets = np.array(globals.full_trainset.targets)\n",
    "        globals.testset = torchvision.datasets.MNIST(data_folder, train=False, download=True,\n",
    "                                transform=transform)\n",
    "    elif globals.dataset == 'tiny_imagenet':\n",
    "        transform = transforms.Compose([\n",
    "            transforms.ToTensor()\n",
    "            #transforms.Normalize((0.5,), (0.5,))  # Normalizes to mean 0.5 and std 0.5 for the single channel\n",
    "        ])\n",
    "        globals.full_trainset = datasets.ImageFolder(root=data_folder + \"tiny-imagenet-200/train\", transform=transform)\n",
    "        globals.testset = datasets.ImageFolder(root=data_folder + \"tiny-imagenet-200/val\", transform=transform)\n",
    "        targets = [sample[1] for sample in globals.full_trainset.samples]\n",
    "        targets = np.array(targets)\n",
    "    else:\n",
    "        raise NotImplementedError(\"unsupported dataset\")\n",
    "    if globals.val_set_size != 0:\n",
    "        # Perform stratified split\n",
    "        train_indices, val_indices = train_test_split(\n",
    "            np.arange(len(targets)),\n",
    "            test_size=globals.val_set_size,\n",
    "            stratify=targets\n",
    "        )\n",
    "    else:\n",
    "        train_indices = np.arange(len(targets))\n",
    "        val_indices = []\n",
    "\n",
    "    # Create subsets\n",
    "    valset = Subset(globals.full_trainset, val_indices)\n",
    "    globals.trainset = Subset(globals.full_trainset, train_indices)\n",
    "\n",
    "    # Define class pairs for each subset\n",
    "    class_pairs = [tuple(range(i*globals.CLASSES_PER_ITER,(i+1)*globals.CLASSES_PER_ITER)) for i in range(globals.ITERATIONS)]\n",
    "    #print(class_pairs)\n",
    "\n",
    "    # Dictionary to hold data loaders for each subset\n",
    "    globals.trainloaders = []\n",
    "    globals.testloaders = []\n",
    "    globals.valloaders = []\n",
    "    subset_indices = []\n",
    "\n",
    "    def compute_class_to_indices(dataset):\n",
    "        class_to_indices = defaultdict(list)\n",
    "        for idx, (_, label) in enumerate(dataset):\n",
    "            class_to_indices[label].append(idx)\n",
    "        return class_to_indices\n",
    "\n",
    "    train_class_to_indices = compute_class_to_indices(globals.trainset)\n",
    "    val_class_to_indices = compute_class_to_indices(valset)\n",
    "    test_class_to_indices = compute_class_to_indices(globals.testset)\n",
    "\n",
    "    # Loop over each class pair\n",
    "    for i, t in enumerate(class_pairs):\n",
    "        # Get indices of images belonging to the specified class pair\n",
    "        subs_ind = [idx for cls in t for idx in train_class_to_indices[cls]]\n",
    "        val_subset_indices = [idx for cls in t for idx in val_class_to_indices[cls]]\n",
    "        test_subset_indices = [idx for cls in t for idx in test_class_to_indices[cls]]\n",
    "\n",
    "        # Create a subset for the current class pair\n",
    "        train_subset = Subset(globals.trainset, subs_ind)\n",
    "        globals.trainloaders.append(DataLoader(train_subset, batch_size=globals.BATCH_SIZE, shuffle=True, pin_memory=True, num_workers = 0))\n",
    "        subset_indices.append(subs_ind)\n",
    "        \n",
    "        val_subset = Subset(valset, val_subset_indices)\n",
    "        globals.valloaders.append(DataLoader(val_subset, batch_size=100, shuffle=False))\n",
    "\n",
    "        test_subset = Subset(globals.testset, test_subset_indices)\n",
    "        globals.testloaders.append(DataLoader(test_subset, batch_size=100, shuffle=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiment(\n",
    "        verbose = False,\n",
    "        stopOnLoss = None,\n",
    "        full_CE = True,\n",
    "        with_OOD = False,\n",
    "        ood_method = 'jigsaw',\n",
    "        kd_loss = 0,\n",
    "        stopOnValAcc = None,\n",
    "        epochs = 1000000,\n",
    "        with_dropout = False,\n",
    "        ogd = False,\n",
    "        dataset = None,\n",
    "        joint_training = False\n",
    "        ):\n",
    "    def _print(*args, **kwargs):\n",
    "        if verbose:\n",
    "            print(*args, **kwargs)\n",
    "    if dataset is not None:\n",
    "        globals.dataset = dataset\n",
    "    else:\n",
    "        dataset = globals.dataset\n",
    "    if dataset == 'tiny_imagenet':\n",
    "        stopOnLoss = 1.3\n",
    "        if joint_training:\n",
    "            globals.CLASSES_PER_ITER = 200\n",
    "            globals.ITERATIONS = 1\n",
    "        else:\n",
    "            globals.CLASSES_PER_ITER = 40\n",
    "            globals.ITERATIONS = 5\n",
    "    elif dataset == 'mnist':\n",
    "        stopOnLoss = 0.03\n",
    "        if joint_training:\n",
    "            globals.CLASSES_PER_ITER = 10\n",
    "            globals.ITERATIONS = 1\n",
    "        else:\n",
    "            globals.CLASSES_PER_ITER = 2\n",
    "            globals.ITERATIONS = 5\n",
    "    if with_OOD:\n",
    "        globals.toggle_OOD(ood_method)\n",
    "    else:\n",
    "        globals.disable_OOD()\n",
    "    initialize_data()\n",
    "    prevModel = None\n",
    "    globals.BATCH_SIZE=4\n",
    "    \n",
    "    globals.WITH_DROPOUT = with_dropout\n",
    "\n",
    "    #[Denis] added code:\n",
    "    Feature_Importance_Eval=Feature_Importance_Evaluations(globals.testloaders, DEVICE)\n",
    "\n",
    "    for i in tqdm(range(globals.ITERATIONS), desc=\"Experiment Progress\"):\n",
    "        if globals.dataset == 'mnist':\n",
    "            model = MnistCNN((i+1)*(globals.CLASSES_PER_ITER+globals.OOD_CLASS))\n",
    "        elif globals.dataset == 'tiny_imagenet':\n",
    "            model = TinyImageNetCNN((i+1)*(globals.CLASSES_PER_ITER+globals.OOD_CLASS))\n",
    "        if prevModel is not None:\n",
    "            with torch.no_grad():\n",
    "                model.copyPrev(prevModel)\n",
    "        train_loader = globals.trainloaders[i]\n",
    "        val_loader = globals.valloaders[i]\n",
    "        if prevModel:\n",
    "            _print(\"CL TRAIN!!\")\n",
    "            train_model_CL(\n",
    "                model,\n",
    "                prevModel,\n",
    "                train_loader,\n",
    "                val_loader,\n",
    "                i,\n",
    "                verbose,\n",
    "                epochs,\n",
    "                True,\n",
    "                kd_loss=kd_loss,\n",
    "                stopOnLoss=stopOnLoss,\n",
    "                stopOnValAcc = stopOnValAcc,\n",
    "                full_CE=full_CE,\n",
    "                ogd=ogd,\n",
    "                )\n",
    "        else:\n",
    "            _print(\"TRAINING!\")\n",
    "            train_model(\n",
    "                model, \n",
    "                train_loader, \n",
    "                val_loader, \n",
    "                verbose, \n",
    "                epochs=epochs, \n",
    "                stopOnLoss=stopOnLoss,\n",
    "                ogd=ogd\n",
    "                )\n",
    "\n",
    "        #[Denis] added code:\n",
    "        Feature_Importance_Eval.Task_Feature_Attribution(model, i)\n",
    "        \n",
    "        if verbose or i == globals.ITERATIONS-1:\n",
    "            _print(\"Starting evaluation\")\n",
    "            _print(\"ITERATION\", i+1)\n",
    "            _print(\"ACCURACIES PER TASK:\")\n",
    "            accumPred = []\n",
    "            all_labels = []\n",
    "            all_embeddings = []\n",
    "            with torch.no_grad():\n",
    "                for j in range(i+1):\n",
    "                    val_loader = globals.testloaders[j]\n",
    "                    val_labels = get_labels(val_loader).to(DEVICE)\n",
    "                    all_labels.append(val_labels)\n",
    "                    model.eval()\n",
    "                    pred, embeddings = model.get_pred_and_embeddings((get_features(val_loader).to(DEVICE)))\n",
    "                    model.train()\n",
    "                    accumPred.append(pred)\n",
    "                    all_embeddings.append(embeddings)\n",
    "                    sliced_pred = pred[:, j*(globals.CLASSES_PER_ITER+globals.OOD_CLASS):(j+1)*(globals.CLASSES_PER_ITER+globals.OOD_CLASS)]\n",
    "                    _, predicted = torch.max(sliced_pred, 1)  # Get the class predictions\n",
    "                    predicted += j*globals.CLASSES_PER_ITER\n",
    "                    correct = (predicted == val_labels).sum().item()  # Count how many were correct\n",
    "                    accuracy = correct / val_labels.size(0)  # Accuracy as a percentage\n",
    "                    _print(str(accuracy), end=' ')\n",
    "            accumPred = torch.cat(accumPred)\n",
    "            all_labels = torch.cat(all_labels)\n",
    "            all_embeddings = torch.cat(all_embeddings)\n",
    "            predicted = []\n",
    "            for x in accumPred:\n",
    "                if globals.OOD_CLASS == 1:\n",
    "                    x_pred = x[[i for i in range(x.size(0)) if (i + 1) % (globals.CLASSES_PER_ITER+1) != 0]]\n",
    "                else:\n",
    "                    x_pred = x\n",
    "                x_pred = torch.softmax(x_pred, dim=-1)\n",
    "                max = 0\n",
    "                for (k, v) in enumerate(x_pred):\n",
    "                    if v > max:\n",
    "                        max = v\n",
    "                        p = k\n",
    "                predicted.append(p)\n",
    "            predicted = torch.tensor(predicted).to(DEVICE)\n",
    "            correct = (predicted == all_labels).sum().item()  # Count how many were correct\n",
    "            accuracy = correct / all_labels.size(0)  # Accuracy as a percentage\n",
    "            _print(\"Accuracy on tasks so far:\", accuracy)\n",
    "            embedding_drift = measure_embedding_drift(all_embeddings, all_labels, model.prev_test_embedding_centers)\n",
    "            _print(\"Average embedding drift based on centroids:\", embedding_drift)\n",
    "            total_confusion, intra_phase_confusion, per_task_confusion = measure_embedding_confusion_knn(all_embeddings, all_labels, k = 300, task=i+1)\n",
    "            _print(\"Total confusion\", total_confusion)\n",
    "            _print(\"Intra-phase confusion\", intra_phase_confusion)\n",
    "            _print(\"Per task confusions\", per_task_confusion)\n",
    "            if verbose and globals.dataset == 'mnist':\n",
    "                plot_confusion_matrix(predicted.cpu(), all_labels.cpu(), list(range(globals.CLASSES_PER_ITER*(i+1))))\n",
    "        prevModel = copy.deepcopy(model)\n",
    "        \n",
    "    #[Denis] added code:\n",
    "    [avg_att_diff,att_diffs,_,_,avg_att_spread,att_spreads]=Feature_Importance_Eval.Get_Feature_Change_Score(prevModel)\n",
    "    _print(\"Average SHAPC values (ordered as tasks):\", att_diffs)\n",
    "    _print(\"Averaged SHAPC value (the smaller the better):\", avg_att_diff)\n",
    "    _print(\"Average attention spread values (ordered as tasks):\", att_spreads)\n",
    "    _print(\"Averaged attention spread value (the bigger the better):\", avg_att_spread)\n",
    "    Feature_Importance_Eval.Save_Random_Picture_Salency() #prints the salcency maps for 1 example by class (first row: image, second row: salency map after training, third row: salency map after training task where class is included)\n",
    "    \n",
    "    return accuracy, total_confusion, intra_phase_confusion, per_task_confusion, embedding_drift, avg_att_diff, avg_att_spread"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiments(n_runs=globals.EXPERIMENT_N_RUNS, *args, **kwargs):\n",
    "    verbose = kwargs.get('verbose', None)\n",
    "    def _print(*args, **kwargs):\n",
    "        if verbose:\n",
    "            print(*args, **kwargs)\n",
    "    def report_stats(data, name):\n",
    "        #print(name, data)\n",
    "        mean = statistics.mean(data)\n",
    "        std = statistics.stdev(data)\n",
    "        print(f\"Mean \" + name + f\" across {n_runs} runs: {mean}\")\n",
    "        print(f\"Standard deviation of \" + name + f\" across {n_runs} runs: {std}\\n\")\n",
    "    accuracies = []\n",
    "    total_confusions = []\n",
    "    intra_phase_confusions = []\n",
    "    per_task_confusions = []\n",
    "    att_diffs = []\n",
    "    embedding_drifts = []\n",
    "    att_spreads = []\n",
    "    for r in range(n_runs):\n",
    "        print(f\"Starting run {r+1}.\")\n",
    "        accuracy, total_confusion, intra_phase_confusion, per_task_confusion, embedding_drift, avg_att_diff, avg_att_spread = run_experiment(*args, **kwargs)\n",
    "        accuracies.append(accuracy)\n",
    "        total_confusions.append(total_confusion)\n",
    "        intra_phase_confusions.append(intra_phase_confusion)\n",
    "        per_task_confusions.append(per_task_confusion)\n",
    "        att_diffs.append(avg_att_diff)\n",
    "        embedding_drifts.append(embedding_drift)\n",
    "        att_spreads.append(avg_att_spread)\n",
    "        print(f\"Run {r} finished with accuracy {accuracy}\")\n",
    "\n",
    "    report_stats(accuracies, \"accuracy\")\n",
    "    report_stats(total_confusions, \"total confusion\")\n",
    "    report_stats(intra_phase_confusions, \"intra-phase confusion\")\n",
    "    report_stats(per_task_confusions, \"per-task confusion\")\n",
    "    report_stats(embedding_drifts, \"embedding drift\")\n",
    "    report_stats(att_diffs, \"attention drift\")\n",
    "    report_stats(att_spreads, \"attention spread\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_experiments()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_experiments(with_dropout=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_experiments(full_CE=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_experiments(with_OOD=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_experiments(kd_loss=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_experiments(ogd=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_experiments(with_dropout=True, kd_loss=1, full_CE=False, with_OOD=True, ogd=True) # best version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_experiments(with_dropout=True, kd_loss=1, full_CE=False, with_OOD=True, ogd=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_experiments(with_dropout=False, kd_loss=1, full_CE=False, with_OOD=True, ogd=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_experiments(with_dropout=True, kd_loss=1, full_CE=False, with_OOD=False, ogd=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training on entire dataset\n",
    "run_experiments(joint_training=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
