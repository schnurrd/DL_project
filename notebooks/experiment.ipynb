{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "import numpy as np\n",
    "import copy\n",
    "import statistics\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "import sys\n",
    "sys.path.append('./../src')\n",
    "import globals\n",
    "from model import Net\n",
    "from training import train_model, train_model_CL\n",
    "from visualizations import plot_embeddings, plot_confusion_matrix\n",
    "from feature_attribution import Feature_Importance_Evaluations\n",
    "from pytorch_utils import get_features, get_labels\n",
    "from embedding_measurements import measure_embedding_confusion_knn, measure_embedding_drift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = globals.SEED\n",
    "DEVICE = globals.DEVICE\n",
    "full_trainset = globals.full_trainset\n",
    "trainset = globals.trainset\n",
    "testset = globals.testset\n",
    "trainloaders = globals.trainloaders\n",
    "valloaders = globals.valloaders\n",
    "testloaders = globals.testloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the two-step process used to prepare the\n",
    "# data for use with the convolutional neural network.\n",
    "\n",
    "# First step is to convert Python Image Library (PIL) format\n",
    "# to PyTorch tensors.\n",
    "\n",
    "# Second step is used to normalize the data by specifying a \n",
    "# mean and standard deviation for each of the three channels.\n",
    "# This will convert the data from [0,1] to [-1,1]\n",
    "\n",
    "# Normalization of data should help speed up conversion and\n",
    "# reduce the chance of vanishing gradients with certain \n",
    "# activation functions.\n",
    "def initialize_data():\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor()\n",
    "        #transforms.Normalize((0.5,), (0.5,))  # Normalizes to mean 0.5 and std 0.5 for the single channel\n",
    "    ])\n",
    "\n",
    "    globals.full_trainset = torchvision.datasets.MNIST('./../data/', train=True, download=True,\n",
    "                                transform=transform)\n",
    "    targets = np.array(globals.full_trainset.targets)\n",
    "\n",
    "    if globals.val_set_size != 0:\n",
    "        # Perform stratified split\n",
    "        train_indices, val_indices = train_test_split(\n",
    "            np.arange(len(targets)),\n",
    "            test_size=0.01,\n",
    "            stratify=targets\n",
    "        )\n",
    "    else:\n",
    "        train_indices = np.arange(len(targets))\n",
    "        val_indices = []\n",
    "\n",
    "    # Create subsets\n",
    "    valset = Subset(globals.full_trainset, val_indices)\n",
    "    globals.trainset = Subset(globals.full_trainset, train_indices)\n",
    "\n",
    "    globals.testset = torchvision.datasets.MNIST('./../data/', train=False, download=True,\n",
    "                                transform=transform)\n",
    "\n",
    "    # Define class pairs for each subset\n",
    "    class_pairs = [tuple(range(i*globals.CLASSES_PER_ITER,(i+1)*globals.CLASSES_PER_ITER)) for i in range(globals.ITERATIONS)]\n",
    "    #print(class_pairs)\n",
    "\n",
    "    # Dictionary to hold data loaders for each subset\n",
    "    globals.trainloaders = []\n",
    "    globals.testloaders = []\n",
    "    globals.valloaders = []\n",
    "    subset_indices = []\n",
    "    # Loop over each class pair\n",
    "    for i, t in enumerate(class_pairs):\n",
    "        # Get indices of images belonging to the specified class pair\n",
    "        subs_ind = [idx for idx, (_, label) in enumerate(globals.trainset) if label in list(t)]\n",
    "        val_subset_indices = [idx for idx, (_, label) in enumerate(valset) if label in list(t)]\n",
    "        test_subset_indices = [idx for idx, (_, label) in enumerate(globals.testset) if label in list(t)]\n",
    "        # Create a subset for the current class pair\n",
    "        train_subset = Subset(globals.trainset, subs_ind)\n",
    "        globals.trainloaders.append(DataLoader(train_subset, batch_size=globals.BATCH_SIZE, shuffle=True, pin_memory=True, num_workers = 0))\n",
    "\n",
    "        subset_indices.append(subs_ind)\n",
    "        \n",
    "        val_subset = Subset(valset, val_subset_indices)\n",
    "        globals.valloaders.append(DataLoader(val_subset, batch_size=500, shuffle=False))\n",
    "\n",
    "        test_subset = Subset(globals.testset, test_subset_indices)\n",
    "        globals.testloaders.append(DataLoader(test_subset, batch_size=500, shuffle=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiment(\n",
    "        verbose = False,\n",
    "        stopOnLoss = 0.02,\n",
    "        full_CE = True,\n",
    "        with_OOD = False,\n",
    "        ood_method = 'jigsaw',\n",
    "        kd_loss = 0,\n",
    "        stopOnValAcc = None,\n",
    "        epochs = 1000000,\n",
    "        with_dropout = False,\n",
    "        ogd = False\n",
    "        ):\n",
    "    def _print(*args, **kwargs):\n",
    "        if verbose:\n",
    "            print(*args, **kwargs)\n",
    "    if with_OOD:\n",
    "        globals.toggle_OOD(ood_method)\n",
    "    else:\n",
    "        globals.disable_OOD()\n",
    "    initialize_data()\n",
    "    prevModel = None\n",
    "    globals.BATCH_SIZE=4\n",
    "    \n",
    "    globals.WITH_DROPOUT = with_dropout\n",
    "\n",
    "    #[Denis] added code:\n",
    "    Feature_Importance_Eval=Feature_Importance_Evaluations(globals.testloaders, DEVICE)\n",
    "\n",
    "    for i in tqdm(range(globals.ITERATIONS), desc=\"Experiment Progress\"):\n",
    "        model = Net((i+1)*(globals.CLASSES_PER_ITER+globals.OOD_CLASS))\n",
    "        if prevModel is not None:\n",
    "            with torch.no_grad():\n",
    "                model.copyPrev(prevModel)\n",
    "        train_loader = globals.trainloaders[i]\n",
    "        val_loader = globals.valloaders[i]\n",
    "        if prevModel:\n",
    "            _print(\"CL TRAIN!!\")\n",
    "            train_model_CL(\n",
    "                model,\n",
    "                prevModel,\n",
    "                train_loader,\n",
    "                val_loader,\n",
    "                i,\n",
    "                verbose,\n",
    "                epochs,\n",
    "                True,\n",
    "                freeze_nonzero_params=False,\n",
    "                l1_loss=0,\n",
    "                ewc_loss=0,\n",
    "                kd_loss=kd_loss,\n",
    "                distance_loss=0,\n",
    "                center_loss=0,\n",
    "                param_reuse_loss=0,\n",
    "                stopOnLoss=stopOnLoss,\n",
    "                stopOnValAcc = stopOnValAcc,\n",
    "                full_CE=full_CE,\n",
    "                ogd=ogd,\n",
    "                )\n",
    "        else:\n",
    "            train_model(\n",
    "                model, \n",
    "                train_loader, \n",
    "                val_loader, \n",
    "                verbose, \n",
    "                epochs=epochs, \n",
    "                l1_loss=0,\n",
    "                stopOnLoss=stopOnLoss,\n",
    "                center_loss =0,\n",
    "                ogd=ogd\n",
    "                )\n",
    "\n",
    "        #[Denis] added code:\n",
    "        Feature_Importance_Eval.Task_Feature_Attribution(model, i)\n",
    "        \n",
    "        if verbose or i == globals.ITERATIONS-1:\n",
    "            _print(\"Starting evaluation\")\n",
    "            _print(\"ITERATION\", i+1)\n",
    "            _print(\"ACCURACIES PER TASK:\")\n",
    "            accumPred = []\n",
    "            all_labels = []\n",
    "            all_embeddings = []\n",
    "            with torch.no_grad():\n",
    "                for j in range(i+1):\n",
    "                    val_loader = globals.testloaders[j]\n",
    "                    val_labels = get_labels(val_loader).to(DEVICE)\n",
    "                    all_labels.append(val_labels)\n",
    "                    model.eval()\n",
    "                    pred, embeddings = model.get_pred_and_embeddings((get_features(val_loader).to(DEVICE)))\n",
    "                    model.train()\n",
    "                    accumPred.append(pred)\n",
    "                    all_embeddings.append(embeddings)\n",
    "                    sliced_pred = pred[:, j*(globals.CLASSES_PER_ITER+globals.OOD_CLASS):(j+1)*(globals.CLASSES_PER_ITER+globals.OOD_CLASS)]\n",
    "                    _, predicted = torch.max(sliced_pred, 1)  # Get the class predictions\n",
    "                    predicted += j*globals.CLASSES_PER_ITER\n",
    "                    correct = (predicted == val_labels).sum().item()  # Count how many were correct\n",
    "                    accuracy = correct / val_labels.size(0)  # Accuracy as a percentage\n",
    "                    _print(str(accuracy), end=' ')\n",
    "            accumPred = torch.cat(accumPred)\n",
    "            all_labels = torch.cat(all_labels)\n",
    "            all_embeddings = torch.cat(all_embeddings)\n",
    "            predicted = []\n",
    "            for x in accumPred:\n",
    "                if globals.OOD_CLASS == 1:\n",
    "                    x_pred = x[[i for i in range(x.size(0)) if (i + 1) % (globals.CLASSES_PER_ITER+1) != 0]]\n",
    "                else:\n",
    "                    x_pred = x\n",
    "                x_pred = torch.softmax(x_pred, dim=-1)\n",
    "                max = 0\n",
    "                for (k, v) in enumerate(x_pred):\n",
    "                    if v > max:\n",
    "                        max = v\n",
    "                        p = k\n",
    "                predicted.append(p)\n",
    "            predicted = torch.tensor(predicted).to(DEVICE)\n",
    "            correct = (predicted == all_labels).sum().item()  # Count how many were correct\n",
    "            accuracy = correct / all_labels.size(0)  # Accuracy as a percentage\n",
    "            _print(\"Accuracy on tasks so far:\", accuracy)\n",
    "\n",
    "            embedding_drift = measure_embedding_drift(all_embeddings, all_labels, model.prev_test_embedding_centers)\n",
    "            _print(\"Average embedding drift based on centroids:\", embedding_drift)\n",
    "            total_confusion, intra_phase_confusion, per_task_confusion = measure_embedding_confusion_knn(all_embeddings, all_labels, k = 1000, task=i+1)\n",
    "            _print(\"Total confusion\", total_confusion)\n",
    "            _print(\"Intra-phase confusion\", intra_phase_confusion)\n",
    "            _print(\"Per task confusions\", per_task_confusion)\n",
    "            if verbose:\n",
    "                plot_confusion_matrix(predicted.cpu(), all_labels.cpu(), list(range(globals.CLASSES_PER_ITER*(i+1))))\n",
    "        prevModel = copy.deepcopy(model)\n",
    "        \n",
    "    #[Denis] added code:\n",
    "    [avg_att_diff,att_diffs,_,_,avg_att_spread,att_spreads]=Feature_Importance_Eval.Get_Feature_Change_Score(prevModel)\n",
    "    _print(\"Average SHAPC values (ordered as tasks):\", att_diffs)\n",
    "    _print(\"Averaged SHAPC value (the smaller the better):\", avg_att_diff)\n",
    "    _print(\"Average attention spread values (ordered as tasks):\", att_spreads)\n",
    "    _print(\"Averaged attention spread value (the bigger the better):\", avg_att_spread)\n",
    "    Feature_Importance_Eval.Save_Random_Picture_Salency() #prints the salcency maps for 1 example by class (first row: image, second row: salency map after training, third row: salency map after training task where class is included)\n",
    "    \n",
    "    return accuracy, total_confusion, intra_phase_confusion, per_task_confusion, embedding_drift, avg_att_diff, avg_att_spread"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiments(n_runs=globals.EXPERIMENT_N_RUNS, *args, **kwargs):\n",
    "    verbose = kwargs.get('verbose', None)\n",
    "    def _print(*args, **kwargs):\n",
    "        if verbose:\n",
    "            print(*args, **kwargs)\n",
    "    def report_stats(data, name):\n",
    "        #print(name, data)\n",
    "        mean = statistics.mean(data)\n",
    "        std = statistics.stdev(data)\n",
    "        print(f\"Mean \" + name + f\" across {n_runs} runs: {mean}\")\n",
    "        print(f\"Standard deviation of \" + name + f\" across {n_runs} runs: {std}\\n\")\n",
    "    accuracies = []\n",
    "    total_confusions = []\n",
    "    intra_phase_confusions = []\n",
    "    per_task_confusions = []\n",
    "    att_diffs = []\n",
    "    embedding_drifts = []\n",
    "    att_spreads = []\n",
    "    for r in range(n_runs):\n",
    "        print(f\"Starting run {r+1}.\")\n",
    "        accuracy, total_confusion, intra_phase_confusion, per_task_confusion, embedding_drift, avg_att_diff, avg_att_spread = run_experiment(*args, **kwargs)\n",
    "        accuracies.append(accuracy)\n",
    "        total_confusions.append(total_confusion)\n",
    "        intra_phase_confusions.append(intra_phase_confusion)\n",
    "        per_task_confusions.append(per_task_confusion)\n",
    "        att_diffs.append(avg_att_diff)\n",
    "        embedding_drifts.append(embedding_drift)\n",
    "        att_spreads.append(avg_att_spread)\n",
    "        _print(f\"Run {r} finished with accuracy {accuracy}\")\n",
    "\n",
    "    report_stats(accuracies, \"accuracy\")\n",
    "    report_stats(total_confusions, \"total confusion\")\n",
    "    report_stats(intra_phase_confusions, \"intra-phase confusion\")\n",
    "    report_stats(per_task_confusions, \"per-task confusion\")\n",
    "    report_stats(embedding_drifts, \"embedding drift\")\n",
    "    report_stats(att_diffs, \"attention drift\")\n",
    "    report_stats(att_spreads, \"attention spread\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_experiments()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_experiments(with_dropout=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_experiments(kd_loss=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_experiments(full_CE=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_experiments(with_OOD=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_experiments(ogd=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_experiments(with_dropout=True, kd_loss=1, full_CE=False, with_OOD=True, ogd=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_experiments(kd_loss=1, full_CE=False, with_OOD=True, ogd=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_experiments(with_dropout=True, kd_loss=1, full_CE=False, ogd=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training on entire dataset\n",
    "globals.ITERATIONS = 1\n",
    "globals.CLASSES_PER_ITER = 10\n",
    "run_experiments()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
